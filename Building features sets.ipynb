{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#%pylab inline\n",
    "import itertools\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_project = Path.home() / Path('Google Drive/Felix')\n",
    "path_data = path_project / Path(\"data\")\n",
    "path_dump = path_project / Path(\"dump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading cdv data\n",
    "file = path_data / Path(\"felix.csv\")\n",
    "with Path.open(file, 'rb') as fp:\n",
    "    cdv = pd.read_csv(fp,  encoding='cp1252',low_memory=False, index_col = 0)\n",
    "# loadind cdv data without format\n",
    "file = path_data / Path(\"felix_ssfmt.csv\")\n",
    "with Path.open(file, 'rb') as fp:\n",
    "    cdv_ssfmt = pd.read_csv(fp,  encoding='cp1252',low_memory=False, index_col = 0)\n",
    "    # loading MergeCommunesEnvi data\n",
    "file = path_data / Path(\"MergeCommunesEnvi.csv\")\n",
    "with Path.open(file, 'rb') as fp:\n",
    "    MergeCommunesEnvi = pd.read_csv(fp,  encoding='cp1252',low_memory=False, sep=';', index_col = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load various variable set\n",
    "filename = path_dump / Path(\"dict_var_groups.sav\")\n",
    "with open(filename, 'rb') as fp:\n",
    "     dict_var_groups = pickle.load(fp)\n",
    "\n",
    "usual_common_scope = dict_var_groups['usual_common_scope']\n",
    "\n",
    "cat_var = dict_var_groups['cat_var']\n",
    "cat_max9_var = dict_var_groups['cat_max9_var']\n",
    "quant_var = dict_var_groups['quant_var']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836 columns after encoding of 155 categorial variables in 572 binary variables (K-1 one hot encoding)\n"
     ]
    }
   ],
   "source": [
    "df = MergeCommunesEnvi.loc[:,:]\n",
    "df.loc[:,cdv_ssfmt.columns] = cdv_ssfmt.loc[:,:]\n",
    "df = df.loc[:,usual_common_scope]\n",
    "df.loc[:,(cat_var & usual_common_scope) - {\"HEUREUX\"}] = cdv.loc[:,(cat_var & usual_common_scope) - {\"HEUREUX\"}]\n",
    "\n",
    "\n",
    "df_dummies = pd.get_dummies(\n",
    "    df, \n",
    "    columns=(cat_var & usual_common_scope) - {\"HEUREUX\"},\n",
    "    dummy_na = True,\n",
    "    drop_first=1\n",
    ")\n",
    "\n",
    "print(f\"{df_dummies.shape[1]} columns after encoding of {len((cat_var & usual_common_scope))-1} categorial \\\n",
    "variables in {len((cat_var & usual_common_scope))-1+df_dummies.shape[1]-df.shape[1]} binary variables \\\n",
    "(K-1 one hot encoding)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_features_sets = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "usual_common_features  = set(df_dummies.columns)\n",
    "dict_features_sets['usual_common_features'] = usual_common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indiv_semi_act_var = dict_var_groups[\"indiv_semi_act_var\"] \n",
    "indiv_act_var = dict_var_groups[\"indiv_act_var\"] \n",
    "admin_semi_act_var = dict_var_groups[\"admin_semi_act_var\"] \n",
    "admin_act_var = dict_var_groups[\"admin_act_var\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ASSOAUTR',\n",
       " 'ASSOCONF',\n",
       " 'ASSOCONS',\n",
       " 'ASSOCULT',\n",
       " 'ASSOENVI',\n",
       " 'ASSOHUMA',\n",
       " 'ASSOJEUN',\n",
       " 'ASSOPARE',\n",
       " 'ASSOPOLI',\n",
       " 'ASSOSPOR',\n",
       " 'ASSOSYND',\n",
       " 'FREQBIBL',\n",
       " 'FREQCINE',\n",
       " 'FREQSPOR',\n",
       " 'FREQTELE',\n",
       " 'NOT_AMIS',\n",
       " 'NOT_COHE',\n",
       " 'NOT_FAMI',\n",
       " 'NOT_LIBR',\n",
       " 'NOT_POLI',\n",
       " 'NOT_PROF',\n",
       " 'RELIGION',\n",
       " 'VACANCES'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scope = (indiv_act_var) & usual_common_scope\n",
    "scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 columns after encoding of 13 categorial variables in 40 binary variables (K-1 one hot encoding)\n"
     ]
    }
   ],
   "source": [
    "df = MergeCommunesEnvi.loc[:,:]\n",
    "df.loc[:,cdv_ssfmt.columns] = cdv_ssfmt.loc[:,:]\n",
    "df = df.loc[:,scope]\n",
    "df.loc[:,(cat_var & scope) - {\"HEUREUX\"}] = cdv.loc[:,(cat_var & scope) - {\"HEUREUX\"}]\n",
    "\n",
    "\n",
    "df_dummies = pd.get_dummies(\n",
    "    df, \n",
    "    columns=(cat_var & scope),\n",
    "    dummy_na = True,\n",
    "    drop_first=1\n",
    ")\n",
    "\n",
    "print(f\"{df_dummies.shape[1]} columns after encoding of {len((cat_var & scope))} categorial \\\n",
    "variables in {len((cat_var & scope))+df_dummies.shape[1]-df.shape[1]} binary variables \\\n",
    "(K-1 one hot encoding)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_act_features = set(df_dummies.columns)\n",
    "dict_features_sets['indiv_act_features'] = indiv_act_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scope correspondong to features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = MergeCommunesEnvi.loc[:,:]\n",
    "df.loc[:,cdv_ssfmt.columns] = cdv_ssfmt.loc[:,:]\n",
    "df = df.loc[:,usual_common_scope]\n",
    "df.loc[:,(cat_var & usual_common_scope) - {\"HEUREUX\"}] = cdv.loc[:,(cat_var & usual_common_scope) - {\"HEUREUX\"}]\n",
    "\n",
    "\n",
    "df = pd.get_dummies(\n",
    "    df, \n",
    "    columns=(cat_var & usual_common_scope) - {\"HEUREUX\"},\n",
    "    dummy_na = True,\n",
    "    drop_first=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11131, 836)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding of \"HEUREUX\" '[nsp]'\n",
    "#df.loc[df[\"HEUREUX\"]==5,\"HEUREUX\"]= None\n",
    "#df = df.loc[np.isfinite(df['HEUREUX']).index,:]\n",
    "\n",
    "# reducing problem to a 2 class classification problem\n",
    "df[\"HEUREUX_CLF\"] = 0\n",
    "df.loc[df[\"HEUREUX\"]==4, \"HEUREUX_CLF\"] = 1\n",
    "df.loc[df[\"HEUREUX\"]==3, \"HEUREUX_CLF\"] = 1\n",
    "df.loc[df[\"HEUREUX\"]==5, \"HEUREUX_CLF\"] = None\n",
    "\n",
    "# treating remaining missing values\n",
    "features = set(df.columns.drop(['HEUREUX', 'HEUREUX_CLF']))\n",
    "df_tmp = df.loc[:,features | {\"HEUREUX_CLF\"}].dropna()\n",
    "\n",
    "X = df_tmp.loc[:,features]\n",
    "y = df_tmp[\"HEUREUX_CLF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number exemple: 10445\n",
      "- training set: 8356\n",
      "- test set: 2089\n",
      "Number of features: p=835\n",
      "Number of class: 2\n",
      "class 0 : 35.1%\n",
      "class 1 : 64.9%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Number exemple: {y.shape[0]}\\n- training set: \\\n",
    "{y_train.shape[0]}\\n- test set: {y_test.shape[0]}\")\n",
    "print(f\"Number of features: p={X_train.shape[1]}\")\n",
    "print(f\"Number of class: {len(np.unique(y))}\")\n",
    "for c in np.unique(y):\n",
    "    print(f\"class {c:0.0f} : {100*np.sum(y==c)/len(y):0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal support of size 20 found in 841.0 s\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "n_features_to_select = 20\n",
    "step = 0.05\n",
    "clf = LogisticRegression(C=1, \n",
    "                         penalty='l1', \n",
    "                         class_weight='balanced',\n",
    "                         random_state=42)\n",
    "selector = RFE(estimator=clf, n_features_to_select=n_features_to_select, step=step)\n",
    "selector.fit(X_train, y_train)\n",
    "print(f\"Optimal support of size {n_features_to_select} found in {time.time() - startTime:0.1f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_20_features = X.loc[:,selector.support_].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_20_features = set(lasso_20_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_features_sets['lasso_20_features'] = lasso_20_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = path_dump / Path(\"dict_features_sets.sav\")\n",
    "with open(filename, 'wb') as fp:\n",
    "     pickle.dump(dict_features_sets,fp,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving dataset data\n",
    "file = path_data / Path(\"dataset.csv\")\n",
    "with Path.open(file, 'w') as fp:\n",
    "    df_tmp.to_csv(fp,  encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
